function [grad_W, grad_b] = Backward(W, b, X, Y, act_h, act_a)
% [grad_W, grad_b] = Backward(W, b, X, Y, act_h, act_a) computes the gradient
% updates to the deep network parameters and returns them in cell arrays
% 'grad_W' and 'grad_b'. This function takes as input:
%   - 'W' and 'b' the network parameters
%   - 'X' and 'Y' the single input data sample and ground truth output vector,
%     of sizes Nx1 and Cx1 respectively
%   - 'act_h' and 'act_a' the network layer pre and post activations when forward
%     forward propogating the input smaple 'X'

% retrieve parameters
num_class = length(Y);  
num_data = length(X);

% initialize gradients for weights and biases, and errors
sizeL = length(W);
grad_W = cell(sizeL, 1);
grad_b = cell(sizeL, 1);
errors = cell(sizeL, 1);

for i = sizeL - 1: -1: 1
end 

% compute gradients of weights and biases
grad_W{1} = (errors{1} * X')';   % for input layer
grad_b{1} = (errors{1})';

% for hidden layers
for i = sizeL: -1: 2
    grad_W{i} = (errors{i} * (act_h{i - 1})')';   
    grad_b{i} = (errors{i})';
end

for i = sizeL: -1: 1'
    if i == sizeL   % output layer
        errors{i} = act_h{i} - act_h{i} .* Y / (Y' * act_h{i});  
    else 
        errors{i} = act_h{i} .* (1 - act_h{i}) .* (W{i + 1} * errors{i + 1});
    end 
    
end 

end
